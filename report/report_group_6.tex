%
% Template for DAS course projects
%
\documentclass[a4paper,11pt,oneside]{book}
\usepackage[latin1]{inputenc}
\usepackage[english]{babel}
\usepackage{amsfonts}
\usepackage{amsmath}
\usepackage{bm}
\usepackage{amssymb,amsmath,color}
\usepackage{cite}
\usepackage{graphicx}
\usepackage{float}
\usepackage{caption}
\usepackage{subcaption}
\usepackage[hidelinks]{hyperref}
\usepackage{cleveref}
\usepackage{booktabs}

\begin{document}
\pagestyle{myheadings}

\def\R{{\mathbb{R}}}
\def\z{{\bm{z}}}
\def\Q{{\bm{Q}}}
\def\r{{\bm{r}}}
\def\p{{\bm{p}}}


%%%%%%%%%%% Cover %%%%%%%%%%%
\thispagestyle{empty}                                                 
\begin{center}                                                            
    \vspace{5mm}
    {\LARGE UNIVERSIT\`A DI BOLOGNA} \\                       
      \vspace{5mm}
\end{center}
\begin{center}
  \includegraphics[scale=.27]{figs/logo_unibo}
\end{center}
\begin{center}
      \vspace{5mm}
      {\LARGE School of Engineering} \\
        \vspace{3mm}
      {\Large Master Degree in Automation Engineering} \\
      \vspace{20mm}
      {\LARGE Distributed Autonomous Systems} \\
      \vspace{5mm}{\Large\textbf{TITLE}}                  
      \vspace{15mm}
\end{center}
\begin{minipage}{0.48\linewidth}
      \raggedright
     {\large Professors:}\\
     \textbf{Giuseppe Notarstefano} \\
     \textbf{Ivano Notarnicola} \\        
%      \vspace{13mm}
\end{minipage}
\begin{minipage}{0.48\linewidth}
      \raggedleft
      {\large Students:}\\
      \textbf{\@ Valerio Costa} \\
      \textbf{\@ Tian Cheng Xia} \\  
\end{minipage}
\begin{center}
\vfill
      {\large Academic year \@2024/2025} \\
\end{center}



\newpage
\thispagestyle{empty}

%%%%%%%%%%% Abstract %%%%%%%%%%%%
\begin{center}
\chapter*{}
\thispagestyle{empty}
{\Huge \textbf{Abstract}}\\
\vspace{15mm}
\end{center}

\tableofcontents \thispagestyle{empty}
% \listoffigures\thispagestyle{empty}

%%%%%%%%%% Introduction %%%%%%%%%%
\chapter*{Introduction}
\addcontentsline{toc}{chapter}{Introduction}
\section*{Motivations} 

\section*{Contributions}


\chapter{Multi-Robot Target Localization}

\section{Gradient tracking with quadratic functions}

The first part of the task consists of implementing the gradient tracking algorithm generalized in $\R^d$ and then experiment with the implementation using quadratic functions, which we define in the usual way as:
\[
      f(\z) = \frac{1}{2} \z^T \Q \z + \r^T \z
      \quad
      \nabla f(\z) = \Q \z + \r
\]
where $\z \in \R^{d}$, $\Q \in \R^{d \times d}$ is positive definite, and $\r \in \R^{d}$.

We analyzed the behavior with quadratic functions through the definition of different problems with different kinds of graph patterns (in particular complete, binomial, cycle, star, and path graph). The configurations we tested are the following:
\begin{itemize}
      \item A small problem ($5$ agents in $\R^3$),
      \item A problem with higher dimensionality ($5$ agents in $\R^{15}$),
      \item A problem with many agents ($15$ agents in $\R^3$), and
      \item A problem with many agents in higher dimensionality ($15$ agents in $\R^{15}$).
\end{itemize}
In addition, we performed a comparison between the distributed gradient tracking algorithm and the centralized one. For compactness in the discussion, in the rest of this report we show the results with a single initialization seed and, if not specified, it indicates that the results are consistent across different initializations. Also, for readability, for quadratic functions we report the distance to the optimum in semi-logarithmic scale instead of the cost itself which can be negative.


\subsection{Comparison between different graph patterns}

For the starting small problem, we can observe from \Cref{fig:quadratic_5_3} a relatively smooth improvement of the cost function and an exponentially decreasing gradient in all cases. Moreover, a result that can be expected and is consistently persistent in all the other experiments is that consensus is reached slightly faster with a complete graph. Next, by experimenting with higher dimensionality, we can observe from \Cref{fig:quadratic_5_15} that the behavior of both the cost and its gradient are very similar to the previous case with the only difference that more iterations are required to reach full convergence, indicating that the dimensionality is marginal in changing the difficulty of the problem. 

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/5_3/distance.pdf} 
            \caption{Distance to optimum}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/5_3/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/5_3/consensus.pdf} 
            \caption{Consensus error}
      \end{subfigure}
      \caption{Quadratic function minimization with $5$ agents in $\R^{3}$}
      \label{fig:quadratic_5_3}
\end{figure}

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/5_15/distance.pdf} 
            \caption{Cost evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/5_15/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      % \hfill
      % \begin{subfigure}[t]{0.46\linewidth}
      %       \centering
      %       \includegraphics[width=\linewidth]{./figs/quadratic/5_15_100/consensus.pdf} 
      %       \caption{Consensus error}
      % \end{subfigure}
      \caption{Quadratic function minimization with $5$ agents in $\R^{15}$}
      \label{fig:quadratic_5_15}
\end{figure}

In the case of many agents with lower dimensionality, we can observe from \Cref{fig:quadratic_15_3} that the cost function does not reach the optimum within the given number of iterations with the configuration using the path graph, indicating that connectivity is important for larger numbers of agents. This can be explained by the fact that, by adding more agents, the overall problem includes more local losses and becomes more difficult to solve in a distributed way. From \Cref{fig:quadratic_15_15}, we observe the same convergence behavior as in the previous case and also confirm that, with higher dimensionality, the problem is not significantly affected.


% Follo
% The following plots represent how the behavior of the algorithm changes between different kinds of networks.

% and higher dimensionality In particular, we observed a much relevant change in terms of iterations required to reach convergence for the case with 15 agents, which needed an extension in the number of iterations to show when i t
% reaches equiili

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/15_3/distance.pdf} 
            \caption{Cost evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/15_3/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      % \hfill
      % \begin{subfigure}[t]{0.46\linewidth}
      %       \centering
      %       \includegraphics[width=\linewidth]{./figs/quadratic/15_3/consensus.pdf} 
      %       \caption{Consensus error}
      % \end{subfigure}
      \caption{Quadratic function minimization with $15$ agents in $\R^{3}$}
      \label{fig:quadratic_15_3}
\end{figure}

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/15_15/distance.pdf} 
            \caption{Cost evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/15_15/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      % \hfill
      % \begin{subfigure}[t]{0.46\linewidth}
      %       \centering
      %       \includegraphics[width=\linewidth]{./figs/quadratic/15_15/consensus.pdf} 
      %       \caption{Consensus error}
      % \end{subfigure}
      \caption{Quadratic function minimization with $15$ agents in $\R^{15}$}
      \label{fig:quadratic_15_15}
\end{figure}


% At last, we experimented with a higher number of iterations to analyze the behavior at convergence. From \Cref{fig:quadratic_15_3_1000}, we can observe that the configuration with a complete graph is the one that converges with the most precise gradient, while the worst performing is the path graph that is the slowest to converge.

% \begin{figure}[tb!]
%       \centering
%       \begin{subfigure}[t]{0.46\linewidth}
%             \centering
%             \includegraphics[width=\linewidth]{./figs/quadratic/15_3/distance.pdf} 
%             \caption{Cost evolution}
%       \end{subfigure}
%       \hfill
%       \begin{subfigure}[t]{0.46\linewidth}
%             \centering
%             \includegraphics[width=\linewidth]{./figs/quadratic/15_3/gradient.pdf} 
%             \caption{Gradient norm evolution}
%       \end{subfigure}
%       % \hfill
%       % \begin{subfigure}[t]{0.46\linewidth}
%       %       \centering
%       %       \includegraphics[width=\linewidth]{./figs/quadratic/15_3/consensus.pdf} 
%       %       \caption{Consensus error}
%       % \end{subfigure}
%       \caption{Quadratic function minimization with $15$ agents in $\R^{3}$ to convergence}
%       \label{fig:quadratic_15_3_1000}
% \end{figure}


\subsection{Comparison with centralized gradient}

Following the previous results, we select the configuration using the complete graph for the comparison with the centralized gradient method. In \Cref{fig:quadratic_centralized_15_3}, we can observe the results with $15$ agents, but the overall behavior is the same for all configurations. It can be seen that, as one could expect, the centralized gradient method is faster to converge compared to a distributed algorithm as it has available all the global information and does not rely on estimates and information exchange with the neighbors.

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/centralized/distance.pdf} 
            \caption{Distance to optimum}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/quadratic/centralized/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Quadratic function minimization with $15$ agents in $\R^{3}$ compared to centralized gradient}
      \label{fig:quadratic_centralized_15_3}
\end{figure}



\section{Cooperative multi-robot target localization}

The second part of the task involves applying the gradient tracking algorithm for estimating the position of $N_T$ fixed targets in a distributed way through $N_R$ tracking robots. Each robot is located at position $\p_i \in \R^2$ and it is assumed that the distance measured from each robot is noisy. Given the positions $\p_i$ and $\p_\tau$ of the $i$-th robot and the $\tau$-th target, respectively, we model the measured noisy distance $d_{i,\tau}$ as follows:
\[
      d_{i,\tau} = \Vert \p_i - \p_\tau \Vert + \varepsilon \cdot \texttt{noise}
\]
where $\texttt{noise}$ is drawn from some distribution and $\varepsilon$ is the noise rate.

The local loss each robot $i$ uses is the following:
\[
      \begin{gathered}
            l_i(\z) = \sum_{\tau=1}^{N_T} \left( d_{i,\tau}^2 - \Vert \z_{\tau} - \p_i \Vert^2 \right)^2
            \quad
            \nabla l_{i}(\z) = (\nabla l_{i,1}(\z_{1}), \dots, \nabla l_{i,N_T}(\z_{N_T}))
            \\
            \nabla l_{i,j}(\z_{j}) = -4 \left( d_{i,j}^2 - \Vert \z_{j} - \p_i \Vert^2 \right) \left( \z_{j} - \p_i \right)
      \end{gathered}
\]
where $\z = (\z_{\tau_1}, \dots, \z_{\tau_{N_T}}) \in \R^{2N_T}$ is the stack of decision variables of robot $i$ containing the estimated positions of the targets $\z_{\tau} \in \R^2$ and $\nabla l_i(\z) \in \R^{2N_T}$ is the concatenation of the gradients computed with respect to each target.

We approach the experimentation of such algorithm by trying different graph patterns and comparing with the centralized gradient method, similarly to the previous case. At first, we evaluated the performance of the algorithm in the following cases, all with the same type of noise:
\begin{itemize}
      \item Network of 5 robots and 1 target,
      \item Network of 5 robots and 3 targets, and
      \item Network of 15 robots and 3 targets.
\end{itemize}

Then, the focus switched to observe how much the performance changes in terms of noise. By fixing the problem configuration, we experimented with varying Gaussian noises, Poisson noises, and noise rates.


\subsection{Comparison between different graph patterns}

In terms of graph pattern, we can observe from \Cref{fig:tracking_5_1} and \Cref{fig:tracking_5_3} that with the same number of robots and increasing number of targets, the number of iterations required to converge is roughly the same. This makes sense as each target is independent to the others and can be tracked in parallel.

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/5_1_2/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/5_1_2/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Tracking with $5$ robots and $1$ target}
      \label{fig:tracking_5_1}
\end{figure}

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/5_3_2/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/5_3_2/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Tracking with $5$ robots and $3$ targets}
      \label{fig:tracking_5_3}
\end{figure}

Instead, by increasing the number of tracking robots, we can see from \Cref{fig:tracking_15_3} that the plateau is reached in fewer number of iteration, which intuitively means that more tracking robots help in reaching a faster convergence.

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/15_3_2/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/15_3_2/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Tracking with $15$ robots and $3$ targets}
      \label{fig:tracking_15_3}
\end{figure}

Moreover, as the total loss is a summation, we must note that the overall loss is higher in the case of more agents or targets, but this does not indicate worse tracking results. We report in \Cref{fig:tracking_avg_error_runs} the average distance between the estimated and real target positions for a fixed configuration with varying number of robots. It can be seen, as intuition would suggest, that on average the tracking error becomes smaller by increasing the number of tracking robots.

\begin{figure}[tb!]
      \centering
      \includegraphics[width=0.46\linewidth]{./figs/tracking/average/avg_tracking.pdf}
      \caption{Average tracking error for different number of robots (average of 5 runs). Vertical bars represent the standard deviation.}
      \label{fig:tracking_avg_error_runs}
\end{figure}

% \begin{table}[ht]
%       \centering
%       \small
%       \caption{Average tracking error at the last iteration}
%       \label{tab:tracking_error}
%       \begin{tabular}{lccc}
%             \toprule
%             & $N_R=5, N_T=1$ & $N_R=5, N_T=3$ & $N_R=15, N_T=3$ \\
%             \midrule
%             \textbf{Complete} & $0.03008$ & $0.00554$ & $0.00500$ \\
%             \textbf{Binomial} & $0.03008$ & $0.00554$ & $0.00500$ \\
%             \textbf{Cycle}    & $0.03008$ & $0.00554$ & $0.00500$ \\
%             \textbf{Star}     & $0.03007$ & $0.00554$ & $0.00500$ \\
%             \textbf{Path}     & $0.03009$ & $0.00553$ & $0.00501$ \\
%             \bottomrule
%       \end{tabulcar}
% \end{table}

Finally, an observation consistent in all experiments is that, as shown in \Cref{fig:tracking_consensus}, the overall behavior of this system is to reach an approximate consensus (i.e., consensus error around $10^{-4}$) in the first few iterations and then optimize the loss while improving and preserving consensus. This can also be observed in \Cref{fig:tracking_animation} where a few frames of the animation of the scenario are shown.

\begin{figure}[tb!]
      \centering
      \includegraphics[width=0.46\linewidth]{./figs/tracking/15_3_2/consensus.pdf} 
      \caption{Tracking with $15$ robots and $3$ targets}
      \label{fig:tracking_consensus}
\end{figure}

\begin{figure}[tb!]
      \centering
      \includegraphics[width=\linewidth]{./figs/tracking/anim.pdf} 
      \caption{Tracking animation with $3$ robots and $1$ target}
      \label{fig:tracking_animation}
\end{figure}


\subsection{Comparison with centralized gradient}

Compared with the centralized gradient algorithm, the plots in \Cref{fig:tracking_centralized_5_3} confirm what we observed before in the case of quadratic functions. The convergence speed is faster and more accurate in a centralized approach, which also results in a lower average tracking error.

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/centralized/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/centralized/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Tracking with $15$ robots and $3$ targets with centralized gradient}
      \label{fig:tracking_centralized_5_3}
\end{figure}


\subsection{Different noises}

From the experiments with different noises, we observed a worsening in performance that is proportional to the amount of noise injected into the distance measurement, implying as one could expect that more noise leads to worse results. This behavior is consistent with noise drawn from different distributions as in \Cref{fig:tracking_gaussian_15_3} and \Cref{fig:tracking_poisson_15_3}, and also when the noise rate is increased as in \Cref{fig:tracking_rates_15_3}.

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/gaussian/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/gaussian/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Tracking with $15$ robots and $3$ targets with noise drawn from Gaussian distributions}
      \label{fig:tracking_gaussian_15_3}
\end{figure}

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/poisson/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/poisson/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Tracking with $15$ robots and $3$ targets with noise drawn from Poisson distributions}
      \label{fig:tracking_poisson_15_3}
\end{figure}

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/rates/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/tracking/rates/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Tracking with $15$ robots and $3$ targets with different rates of Gaussian noise}
      \label{fig:tracking_rates_15_3}
\end{figure}




\cleardoublepage
\chapter{Aggregative Optimization for Multi-Robot Systems}


\section{Problem formulation}

Given $N$ agents with positions $\z_i$ and each associated to a private target $\r_i$, we want to solve the problem of positioning the agents close to its own target while staying close to the rest of the agents.

We solve this optimization problem using the following loss:
\[
      l_i(\z_i, \sigma(\z)) = \gamma_1 \frac{1}{2} \Vert \z_i - \r_i \Vert^2 + \gamma_2 \frac{1}{2} \Vert \z_i - \sigma(\z) \Vert^2
\]
where $\gamma_1$ weighs the importance of being close to the targets and $\gamma_2$ weighs the importance of being tight to the fleet.

In order to compute the global aggregation function $\sigma(\z)$, each agent contributes to the barycenter as the expression of the linear function $\phi_i$:
\[
      \phi_i(\z_i) = \alpha_i \z_i 
      \qquad 
      \sigma(\z) = \frac{1}{N} \sum_{i}^{N} \phi_i(\z_i)
\]
With $\alpha_i = 1 \forall i$ we obtain the standard formulation of the problem in which $\sigma(\z)$ represents the barycenter of the fleet. With different $\alpha_i$ (with $\sum_i^N \alpha_i = N$), we obtain a $\sigma(\z)$ that is a weighted average and can be interpreted as a barycenter that is based towards specified agents.

\section{Comparison with different graph patterns}

As in the previous task, we first experiment this setup with different graph patterns and different number of agents.

From the plots in \Cref{fig:positioning_5}, we can observe an identical trend in terms of loss and gradient for every graph pattern we have experimented with. In all cases, we can observe that the algorithm converges, and also, visually, we can see that the final positions of the agents tends to reach an expected behavior.

By adding more agents, we cannot detect any relevant changes in behavior as they all reach convergence in the same way as in the previous experiment. The only thing we can underline is that the trend for the path graph has a little variation at convergence, most likely due to numerical instability.

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/aggregative/few_agents/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/aggregative/few_agents/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/aggregative/plain_anim/anim.pdf} 
            \caption{Animation frames. Dashed lines connect robots to the private targets and solid lines are the trajectories.}
      \end{subfigure}
      \caption{Positioning with $5$ robots}
      \label{fig:positioning_5}
\end{figure}

\begin{figure}[tb!]
      \centering
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/aggregative/more_agents/loss.pdf} 
            \caption{Loss evolution}
      \end{subfigure}
      \hfill
      \begin{subfigure}[t]{0.46\linewidth}
            \centering
            \includegraphics[width=\linewidth]{./figs/aggregative/more_agents/gradient.pdf} 
            \caption{Gradient norm evolution}
      \end{subfigure}
      \caption{Positioning with $15$ robots}
      \label{fig:positioning_15}
\end{figure}



\section{Comparison with different loss configurations}

In terms of different loss functions, we experiment our formulation with different weights to prioritize target vicinity (higher $\gamma_1$), barycenter vicinity (higher $\gamma_2$), and different agents' importance (different $\alpha_i$). Results are presented in \Cref{fig:anim_target}, \Cref{fig:anim_barycenter}, and \Cref{fig:anim_importance}, respectively. In all cases the positions at convergence are intuitively the expected ones, showing that the algorithm allows many degrees of freedom.

\begin{figure}[tb!]
      \centering
      \includegraphics[width=\linewidth]{./figs/aggregative/target_anim/anim.pdf} 
      \caption{Frames with $5$ robots and loss that prioritizes the private targets. Dashed lines connect robots to the targets and solid lines are the trajectories.}
      \label{fig:anim_target}
\end{figure}

\begin{figure}[tb!]
      \centering
      \includegraphics[width=\linewidth]{./figs/aggregative/barycenter_anim/anim.pdf} 
      \caption{Frames with $5$ robots and loss that prioritizes the barycenter. Dashed lines connect robots to the targets and solid lines are the trajectories.}
      \label{fig:anim_barycenter}
\end{figure}

\begin{figure}[tb!]
      \centering
      \includegraphics[width=\linewidth]{./figs/aggregative/importance_anim/anim.pdf} 
      \caption{Frames with $5$ robots and barycenter that prioritizes robot $0$. Dashed lines connect robots to the targets and solid lines are the trajectories.}
      \label{fig:anim_importance}
\end{figure}



\chapter*{Conclusions}
\addcontentsline{toc}{chapter}{Conclusions} 



\bibliography{bibfile}{}
\bibliographystyle{plain}
\addcontentsline{toc}{chapter}{Bibliography}


\end{document}